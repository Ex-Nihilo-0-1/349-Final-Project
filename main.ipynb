{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests as r\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "import copy\n",
    "import misc\n",
    "import data_processing\n",
    "from importlib import reload\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "reload(data_processing)\n",
    "reload(misc)\n",
    "pd.set_option('display.max_columns', 100)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove Rows with too Many Missing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data_processing.read_data(\"raw_data_long.csv\")\n",
    "\n",
    "data = list(filter(lambda d: len(d.keys()) == len(list(filter(None, list(d.values())))) , data))\n",
    "with open('training_data_long.csv', 'w') as csvfile:\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=list(data[0].keys()))\n",
    "    writer.writeheader()\n",
    "    writer.writerows(data)\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Clean Data Using Pandas (Removes Bad Characters in Column Titles, Removes Population Column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "with open('training_data_long.csv', 'r') as file:\n",
    "    csv_reader = csv.DictReader(file)\n",
    "    data = list(csv_reader)\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#columns = list(df.columns)\n",
    "#columns = list(map(lambda x: re.sub(r\"^\\s\", \"\", x), columns))\n",
    "#columns = list(map(lambda x: re.sub(r\"^\\s\", \"\", x), columns))\n",
    "#columns = list(map(lambda x: re.sub(r\"\\s$\", \"\", x), columns))\n",
    "#columns = list(map(lambda x: re.sub(r\"\\s$\", \"\", x), columns))\n",
    "#columns = list(map(lambda x: re.sub(r\"\\s?.?/$\", \"\", x), columns))\n",
    "columns = df.columns\n",
    "populations = [df['Population (persons) 1/YR-1'],\n",
    "               df['Population (persons) 1/YR-2'],\n",
    "               df['Population (persons) 1/YR-3']]\n",
    "#df.drop(df[columns[-2]], axis = 1, inplace= True)\n",
    "#df[columns[-2]] = population.iloc[:,0]\n",
    "df.drop(columns = \"Population (persons) 1/YR-1\", inplace=True)\n",
    "df.drop(columns = \"Population (persons) 1/YR-2\", inplace=True)\n",
    "df.drop(columns = \"Population (persons) 1/YR-3\", inplace=True)\n",
    "df.drop(columns = \" Population (persons) 3/YR-1\", inplace=True)\n",
    "df.drop(columns = \" Population (persons) 3/YR-2\", inplace=True)\n",
    "df.drop(columns = \" Population (persons) 3/YR-3\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adjust for inflation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in df.index:\n",
    "    try:\n",
    "        year = int(df.iloc[i,1])\n",
    "        df.iloc[i, 4:] = df.iloc[i, 4:].map(lambda x: int(x) * misc.cpi_dict[year])\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        pass\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add Popultation Column Back"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Population YR-1\"] = populations[0]\n",
    "df[\"Population YR-2\"] = populations[1]\n",
    "df[\"Population YR-3\"] = populations[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalize Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in df.columns:\n",
    "    try:\n",
    "        if column != \"Year\":\n",
    "            df[column] = df[column].map(lambda x: int(x))\n",
    "            x_max = max(list(df[column]))\n",
    "            x_min = min(list(df[column]))\n",
    "            df[column] = df[column].map(lambda x: (x - x_min)/(x_max - x_min))\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "df.to_csv(\"training_data_long.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA for Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D \n",
    "\n",
    "%matplotlib widget \n",
    "\n",
    "target, data = data_processing.read_data(\"training_data_long.csv\", mode = \"List\")\n",
    "print(data.shape)\n",
    "\n",
    "pca = PCA()\n",
    "pca.fit(data)\n",
    "#print(pca.explained_variance_ratio_)\n",
    "pca.components_\n",
    "transformed = pca.transform(data)\n",
    "xs=transformed[:,0]\n",
    "ys=transformed[:,1]\n",
    "zs =transformed[:,2]\n",
    "\n",
    "fig = plt.figure(figsize=(10, 10)) \n",
    "Axes3D(fig) \n",
    "ax = fig.add_subplot(projection='3d')\n",
    "fig.patch.set_facecolor('white')\n",
    "target = np.array(target)\n",
    "indices = np.where(target == 'D')\n",
    "ax.scatter(xs[indices], ys[indices], zs[indices], c=\"Blue\", s=0.05, alpha = 0.3)\n",
    "indices = np.where(target == 'R')\n",
    "ax.scatter(xs[indices], ys[indices], zs[indices], c=\"Red\", s=0.05, alpha = 0.3)\n",
    "\n",
    "plt.xlabel(\"First Principal Component\",fontsize=14)\n",
    "plt.ylabel(\"Second Principal Component\",fontsize=14)\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data, Split Data into Training/Valid/Test Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "dict_list = data_processing.read_data(\"training_data_long.csv\")\n",
    "data = []\n",
    "for _ in dict_list:\n",
    "    data += [list(_.values())[1:]]\n",
    "random.shuffle(data)\n",
    "valid_set = data[-1000:]\n",
    "test_set = data[:1000]\n",
    "training_set = data[1000:-1000]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Net Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import training\n",
    "reload(training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn = training.get_nn()\n",
    "nn = training.train(nn, training_set, 0.0002, 1000, 10000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_printoptions(edgeitems=100)\n",
    "tensors = data_to_tensor(training_set)\n",
    "data = tensors.data\n",
    "labels = tensors.labels\n",
    "torch.sum(torch.vmap(torch.argmax)(labels) == torch.vmap(torch.argmax)(nn(data)))/len(labels)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
